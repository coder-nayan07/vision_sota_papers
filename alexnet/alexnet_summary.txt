
Modified AlexNet for CIFAR-10

#----------------HYPERPARAMETERS-----------------------------------------
    Optimizer:Adam
    Learning Rate: 0.001
    Batch Size: 128
    Dropout Rate: 0.5 (default value used in the classifier)
    Activation Function: ReLU (Rectified Linear Unit)
    Epochs: 40

#--------------------ARCHITECTURE-----------------------------------------
The architecture consists of 6 learnable layers: 4 convolutional and 2 in the classifier. It is designed for 32x32x3 input images.
INPUT: [32x32x3] Image
    CONV-1: 64 filters, 5x5, stride 1, padding 2.
        Post-Activation: ReLU
        Pooling: Max-Pooling (3x3, stride 2, padding 1)
        Normalization: Local Response Normalization (LRN)
        Output Volume: [16x16x64]

    CONV-2: 64 filters, 5x5, stride 1, padding 2.
        Post-Activation: ReLU
        Normalization: Local Response Normalization (LRN)
        Pooling: Max-Pooling (3x3, stride 2, padding 1)
        Output Volume: [8x8x64]

    CONV-3: 64 filters, 3x3, stride 1, padding 1.
        Post-Activation: ReLU
        Output Volume: [8x8x64]

    CONV-4: 32 filters, 3x3, stride 1, padding 1.
        Post-Activation: ReLU
        Output Volume: [8x8x32]

Flattening the output of CONV-4 results in a vector of size 8 * 8 * 32 = 2048
    FC-1 (Fully Connected):
        Regularization: Dropout (p=0.5)
        Flatten: The input is flattened.
        Linear: 2048 input features, 10 output features (for 10 classes).

    OUTPUT:
        Loss Function: Cross-Entropy Loss (which includes Softmax).

#---------------------DATASET------------------------------------------------

    Name: CIFAR-10
    Size: 50,000 training images, 10,000 testing images.
    Image Dimensions: 32x32x3
    Classes: 10 (e.g., airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).

#-----------------------RESULTS----------------------------------------------

    refer to the log files


